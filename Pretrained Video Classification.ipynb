{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9455528,"sourceType":"datasetVersion","datasetId":5747912},{"sourceId":9539216,"sourceType":"datasetVersion","datasetId":5810686}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nimport keras\n\ndataset_path = os.listdir('/kaggle/input/shop-dataset/Shop DataSet')\n\nlabel_types = os.listdir('/kaggle/input/shop-dataset/Shop DataSet')\nprint (label_types) ","metadata":{"execution":{"iopub.status.busy":"2024-10-03T13:10:58.028197Z","iopub.execute_input":"2024-10-03T13:10:58.029015Z","iopub.status.idle":"2024-10-03T13:10:58.039691Z","shell.execute_reply.started":"2024-10-03T13:10:58.028962Z","shell.execute_reply":"2024-10-03T13:10:58.038705Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"['non shop lifters', 'shop lifters']\n","output_type":"stream"}]},{"cell_type":"code","source":"rooms = []\n\nfor item in dataset_path:\n # Get all the file names\n all_rooms = os.listdir('/kaggle/input/shop-dataset/Shop DataSet' + '/' +item)\n\n # Add them to the list\n for room in all_rooms:\n    rooms.append((item, str('/kaggle/input/shop-dataset/Shop DataSet' + '/' +item) + '/' + room))\n    \n# Build a dataframe        \ndataSet_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\nprint(dataSet_df.head())\nprint(dataSet_df.tail())","metadata":{"execution":{"iopub.status.busy":"2024-10-03T13:11:00.843123Z","iopub.execute_input":"2024-10-03T13:11:00.843762Z","iopub.status.idle":"2024-10-03T13:11:00.856999Z","shell.execute_reply.started":"2024-10-03T13:11:00.843725Z","shell.execute_reply":"2024-10-03T13:11:00.856108Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"                tag                                         video_name\n0  non shop lifters  /kaggle/input/shop-dataset/Shop DataSet/non sh...\n1  non shop lifters  /kaggle/input/shop-dataset/Shop DataSet/non sh...\n2  non shop lifters  /kaggle/input/shop-dataset/Shop DataSet/non sh...\n3  non shop lifters  /kaggle/input/shop-dataset/Shop DataSet/non sh...\n4  non shop lifters  /kaggle/input/shop-dataset/Shop DataSet/non sh...\n              tag                                         video_name\n850  shop lifters  /kaggle/input/shop-dataset/Shop DataSet/shop l...\n851  shop lifters  /kaggle/input/shop-dataset/Shop DataSet/shop l...\n852  shop lifters  /kaggle/input/shop-dataset/Shop DataSet/shop l...\n853  shop lifters  /kaggle/input/shop-dataset/Shop DataSet/shop l...\n854  shop lifters  /kaggle/input/shop-dataset/Shop DataSet/shop l...\n","output_type":"stream"}]},{"cell_type":"code","source":"df = dataSet_df.loc[:,['video_name','tag']]\ndf\ndf.to_csv('dataSet.csv')","metadata":{"execution":{"iopub.status.busy":"2024-10-03T13:11:04.217752Z","iopub.execute_input":"2024-10-03T13:11:04.218590Z","iopub.status.idle":"2024-10-03T13:11:04.231141Z","shell.execute_reply.started":"2024-10-03T13:11:04.218548Z","shell.execute_reply":"2024-10-03T13:11:04.230267Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"dataSet_df = pd.read_csv(\"dataSet.csv\")\nprint(f\"Total videos in dataSet: {len(dataSet_df)}\")\ndataSet_df.sample(10)  ","metadata":{"execution":{"iopub.status.busy":"2024-10-03T13:11:06.865522Z","iopub.execute_input":"2024-10-03T13:11:06.866201Z","iopub.status.idle":"2024-10-03T13:11:06.881873Z","shell.execute_reply.started":"2024-10-03T13:11:06.866162Z","shell.execute_reply":"2024-10-03T13:11:06.880916Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Total videos in dataSet: 855\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"     Unnamed: 0                                         video_name  \\\n537         537  /kaggle/input/shop-dataset/Shop DataSet/shop l...   \n487         487  /kaggle/input/shop-dataset/Shop DataSet/non sh...   \n720         720  /kaggle/input/shop-dataset/Shop DataSet/shop l...   \n821         821  /kaggle/input/shop-dataset/Shop DataSet/shop l...   \n468         468  /kaggle/input/shop-dataset/Shop DataSet/non sh...   \n475         475  /kaggle/input/shop-dataset/Shop DataSet/non sh...   \n216         216  /kaggle/input/shop-dataset/Shop DataSet/non sh...   \n332         332  /kaggle/input/shop-dataset/Shop DataSet/non sh...   \n360         360  /kaggle/input/shop-dataset/Shop DataSet/non sh...   \n789         789  /kaggle/input/shop-dataset/Shop DataSet/shop l...   \n\n                  tag  \n537      shop lifters  \n487  non shop lifters  \n720      shop lifters  \n821      shop lifters  \n468  non shop lifters  \n475  non shop lifters  \n216  non shop lifters  \n332  non shop lifters  \n360  non shop lifters  \n789      shop lifters  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>video_name</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>537</th>\n      <td>537</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/shop l...</td>\n      <td>shop lifters</td>\n    </tr>\n    <tr>\n      <th>487</th>\n      <td>487</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/non sh...</td>\n      <td>non shop lifters</td>\n    </tr>\n    <tr>\n      <th>720</th>\n      <td>720</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/shop l...</td>\n      <td>shop lifters</td>\n    </tr>\n    <tr>\n      <th>821</th>\n      <td>821</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/shop l...</td>\n      <td>shop lifters</td>\n    </tr>\n    <tr>\n      <th>468</th>\n      <td>468</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/non sh...</td>\n      <td>non shop lifters</td>\n    </tr>\n    <tr>\n      <th>475</th>\n      <td>475</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/non sh...</td>\n      <td>non shop lifters</td>\n    </tr>\n    <tr>\n      <th>216</th>\n      <td>216</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/non sh...</td>\n      <td>non shop lifters</td>\n    </tr>\n    <tr>\n      <th>332</th>\n      <td>332</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/non sh...</td>\n      <td>non shop lifters</td>\n    </tr>\n    <tr>\n      <th>360</th>\n      <td>360</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/non sh...</td>\n      <td>non shop lifters</td>\n    </tr>\n    <tr>\n      <th>789</th>\n      <td>789</td>\n      <td>/kaggle/input/shop-dataset/Shop DataSet/shop l...</td>\n      <td>shop lifters</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df = dataSet_df.copy()","metadata":{"execution":{"iopub.status.busy":"2024-10-03T13:11:10.367148Z","iopub.execute_input":"2024-10-03T13:11:10.368051Z","iopub.status.idle":"2024-10-03T13:11:10.372292Z","shell.execute_reply.started":"2024-10-03T13:11:10.368006Z","shell.execute_reply":"2024-10-03T13:11:10.371401Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\n\n\ndef extract_and_sample_frames(video_path, num_frames=16):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Calculate frame sampling interval\n    sample_interval = max(1, total_frames // num_frames)\n\n    count = 0\n    while len(frames) < num_frames and cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Sample frames at regular intervals\n        if count % sample_interval == 0:\n            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(frame_rgb)\n\n        count += 1\n\n    # If fewer frames are extracted, pad with repeated frames\n    while len(frames) < num_frames:\n        frames.append(frames[-1])\n\n    cap.release()\n    return frames","metadata":{"execution":{"iopub.status.busy":"2024-10-03T13:11:12.935879Z","iopub.execute_input":"2024-10-03T13:11:12.936606Z","iopub.status.idle":"2024-10-03T13:11:12.943954Z","shell.execute_reply.started":"2024-10-03T13:11:12.936566Z","shell.execute_reply":"2024-10-03T13:11:12.943034Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\nimport torch\n\n# Load pre-trained video transformer\nfeature_extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base\")\nmodel = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base\")\n\n# Preprocess frames\ndef preprocess_video_frames(video_path):\n    frames = extract_and_sample_frames(video_path, num_frames=16)\n    inputs = feature_extractor(frames, return_tensors=\"pt\")\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2024-10-03T13:12:01.171668Z","iopub.execute_input":"2024-10-03T13:12:01.172524Z","iopub.status.idle":"2024-10-03T13:12:05.589950Z","shell.execute_reply.started":"2024-10-03T13:12:01.172483Z","shell.execute_reply":"2024-10-03T13:12:05.589017Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm  # Import tqdm for progress bar\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.utils import resample\n\n# Assuming df is your DataFrame with video paths and labels\nvideo_paths = df['video_name'].tolist()  # List of video paths\nlabels = df['tag'].tolist()  # List of labels\n\n# Define a mapping from string labels to numeric labels\nlabel_mapping = {\n    'shop lifters': 1,\n    'non shop lifters': 0\n}\n\nnumeric_labels = [label_mapping[label] for label in labels]\n\n# Separate the minority and majority classes\nminority_class = min(Counter(numeric_labels), key=Counter(numeric_labels).get)\nmajority_class = max(Counter(numeric_labels), key=Counter(numeric_labels).get)\n\nminority_indices = [i for i, label in enumerate(numeric_labels) if label == minority_class]\nmajority_indices = [i for i, label in enumerate(numeric_labels) if label == majority_class]\n\n# Oversample the minority class (duplicate minority examples)\nminority_oversampled_indices = resample(minority_indices, replace=True, n_samples=len(majority_indices), random_state=42)\n\n# Combine the new dataset indices\nbalanced_indices = majority_indices + minority_oversampled_indices\nbalanced_video_paths = [video_paths[i] for i in balanced_indices]\nbalanced_labels = [numeric_labels[i] for i in balanced_indices]\n\n# Preprocess all videos\ninputs_list = []\nlabels_list = []\n\n# Use tqdm to wrap the zip iterator for progress tracking\nfor video_path, label in tqdm(zip(balanced_video_paths, balanced_labels), total=len(balanced_video_paths), desc=\"Processing videos\"):\n    # Preprocess video frames (this should return a tensor or a dict containing a tensor)\n    inputs = preprocess_video_frames(video_path)  # This should return something like {'pixel_values': <tensor>}\n\n    # Assuming 'inputs' is a dict and 'pixel_values' is the key holding the video frames tensor\n    if 'pixel_values' in inputs:\n        pixel_values = inputs['pixel_values']\n\n        # Ensure that pixel_values is a tensor and has consistent dimensions\n        if isinstance(pixel_values, torch.Tensor):\n            inputs_list.append(pixel_values)\n        else:\n            print(f\"Error: Expected tensor for pixel values, got {type(pixel_values)}\")\n            continue  # Skip if the input is invalid\n    else:\n        print(f\"Error: 'pixel_values' not found in inputs for {video_path}\")\n        continue  # Skip if 'pixel_values' is not found\n\n    # Append the corresponding label\n    labels_list.append(label)\n\n# Ensure that there are valid inputs before stacking\nif inputs_list:\n    # Stack all video tensors into a single tensor\n    inputs_tensor = torch.cat(inputs_list, dim=0)  # Concatenate along the batch dimension\n    # Convert the labels list to a tensor\n    labels_tensor = torch.tensor(labels_list)\n\n    print(f\"Inputs tensor shape: {inputs_tensor.shape}\")\n    print(f\"Labels tensor shape: {labels_tensor.shape}\")\nelse:\n    print(\"No valid inputs were processed.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-03T13:12:33.693873Z","iopub.execute_input":"2024-10-03T13:12:33.694266Z","iopub.status.idle":"2024-10-03T13:30:29.050973Z","shell.execute_reply.started":"2024-10-03T13:12:33.694229Z","shell.execute_reply":"2024-10-03T13:30:29.049965Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Processing videos: 100%|██████████| 1062/1062 [17:51<00:00,  1.01s/it]\n","output_type":"stream"},{"name":"stdout","text":"Inputs tensor shape: torch.Size([1062, 16, 3, 224, 224])\nLabels tensor shape: torch.Size([1062])\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\n\n# First, ensure tensors have the correct shapes\nprint(f\"Inputs tensor shape: {inputs_tensor.shape}\")\nprint(f\"Labels tensor shape: {labels_tensor.shape}\")\n\n# Create a dataset using TensorDataset\ndataset = TensorDataset(inputs_tensor, labels_tensor)\n\n# Define the split proportions\ntrain_ratio = 0.75\nvalid_ratio = 0.10\ntest_ratio = 0.15\n\n# Calculate the lengths of each split\ndataset_size = len(dataset)\ntrain_size = int(train_ratio * dataset_size)\nvalid_size = int(valid_ratio * dataset_size)\ntest_size = dataset_size - train_size - valid_size\n\n# Split the dataset into training, validation, and test sets\ntrain_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n\n# Create dataloaders for each split\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n\nprint(f\"Train size: {train_size}, Valid size: {valid_size}, Test size: {test_size}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-03T13:30:45.628972Z","iopub.execute_input":"2024-10-03T13:30:45.629934Z","iopub.status.idle":"2024-10-03T13:30:46.275010Z","shell.execute_reply.started":"2024-10-03T13:30:45.629892Z","shell.execute_reply":"2024-10-03T13:30:46.274117Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Inputs tensor shape: torch.Size([1062, 16, 3, 224, 224])\nLabels tensor shape: torch.Size([1062])\nTrain size: 796, Valid size: 106, Test size: 160\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n\n# Check if a GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Move model to the GPU\nmodel = model.to(device)\n\n# Create a dataset and dataloader\ntrain_dataset = TensorDataset(inputs_tensor, labels_tensor)  # Assuming you've already split your data\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n\n# Assuming you have a validation dataset\nvalid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)\n\n# Define optimizer and loss function\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Function to calculate accuracy\ndef calculate_accuracy(preds, labels):\n    _, predicted_classes = torch.max(preds, 1)\n    correct_predictions = (predicted_classes == labels).sum().item()\n    return correct_predictions / labels.size(0)\n\n# Training loop\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    model.train()\n    total_train_loss = 0\n    total_train_acc = 0\n\n    # Training\n    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n        inputs, labels = batch\n        \n        # Move inputs and labels to the GPU\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(pixel_values=inputs)\n        loss = loss_fn(outputs.logits, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Accumulate loss and accuracy\n        total_train_loss += loss.item()\n        total_train_acc += calculate_accuracy(outputs.logits, labels)\n    \n    # Calculate average loss and accuracy for the epoch\n    avg_train_loss = total_train_loss / len(train_loader)\n    avg_train_acc = total_train_acc / len(train_loader)\n\n    # Validation\n    model.eval()\n    total_valid_loss = 0\n    total_valid_acc = 0\n    with torch.no_grad():\n        for batch in tqdm(valid_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n            inputs, labels = batch\n            \n            # Move inputs and labels to the GPU\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass\n            outputs = model(pixel_values=inputs)\n            loss = loss_fn(outputs.logits, labels)\n\n            # Accumulate validation loss and accuracy\n            total_valid_loss += loss.item()\n            total_valid_acc += calculate_accuracy(outputs.logits, labels)\n\n    # Calculate average validation loss and accuracy for the epoch\n    avg_valid_loss = total_valid_loss / len(valid_loader)\n    avg_valid_acc = total_valid_acc / len(valid_loader)\n\n    # Print the results for the current epoch\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    print(f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {avg_train_acc:.4f}\")\n    print(f\"Validation Loss: {avg_valid_loss:.4f}, Validation Accuracy: {avg_valid_acc:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T13:31:17.619227Z","iopub.execute_input":"2024-10-03T13:31:17.620040Z","iopub.status.idle":"2024-10-03T14:07:31.304111Z","shell.execute_reply.started":"2024-10-03T13:31:17.619975Z","shell.execute_reply":"2024-10-03T14:07:31.303015Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1/5: 100%|██████████| 133/133 [07:03<00:00,  3.19s/it]\nValidation Epoch 1/5: 100%|██████████| 14/14 [00:12<00:00,  1.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\nTraining Loss: 0.3553, Training Accuracy: 0.8145\nValidation Loss: 0.0106, Validation Accuracy: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2/5: 100%|██████████| 133/133 [07:01<00:00,  3.17s/it]\nValidation Epoch 2/5: 100%|██████████| 14/14 [00:12<00:00,  1.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5\nTraining Loss: 0.0531, Training Accuracy: 0.9840\nValidation Loss: 0.0598, Validation Accuracy: 0.9821\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3/5: 100%|██████████| 133/133 [07:01<00:00,  3.17s/it]\nValidation Epoch 3/5: 100%|██████████| 14/14 [00:12<00:00,  1.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5\nTraining Loss: 0.0211, Training Accuracy: 0.9944\nValidation Loss: 0.0009, Validation Accuracy: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 4/5: 100%|██████████| 133/133 [07:01<00:00,  3.17s/it]\nValidation Epoch 4/5: 100%|██████████| 14/14 [00:12<00:00,  1.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5\nTraining Loss: 0.0004, Training Accuracy: 1.0000\nValidation Loss: 0.0002, Validation Accuracy: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 5/5: 100%|██████████| 133/133 [07:00<00:00,  3.16s/it]\nValidation Epoch 5/5: 100%|██████████| 14/14 [00:12<00:00,  1.10it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5\nTraining Loss: 0.0002, Training Accuracy: 1.0000\nValidation Loss: 0.0001, Validation Accuracy: 1.0000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\n\n# Define a mapping from the model's output to labels\nlabel_mapping = {1: 'Shoplifter', 0: 'Non-Shoplifter'}\n\n# Preprocessing function (same as used in training)\ndef preprocess_external_video(video_path):\n    # Assuming you have a function 'extract_and_sample_frames' to extract video frames\n    frames = extract_and_sample_frames(video_path, num_frames=16)  # Adjust the number of frames as needed\n    inputs = feature_extractor(frames, return_tensors=\"pt\")\n    return inputs['pixel_values']\n\n# Prediction function\ndef predict_shoplifter(video_path, model, device):\n    # Preprocess the external video\n    pixel_values = preprocess_external_video(video_path)\n\n    # Move the input to the device (GPU/CPU)\n    pixel_values = pixel_values.to(device)\n\n    # Set model to evaluation mode\n    model.eval()\n\n    # Run the model with no gradient calculation (for inference)\n    with torch.no_grad():\n        # Make prediction\n        outputs = model(pixel_values=pixel_values)\n        logits = outputs.logits\n\n    # Get the predicted class (0: Non-Shoplifter, 1: Shoplifter)\n    predicted_class = torch.argmax(logits, dim=1).item()\n\n    # Map the predicted class to the actual label\n    prediction_label = label_mapping[predicted_class]\n\n    return prediction_label\n\n# Example usage\nexternal_video_path = \"/kaggle/input/shop-dataset/Shop DataSet/shop lifters/shop_lifter_0.mp4\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Make a prediction for an external video\nprediction = predict_shoplifter(external_video_path, model, device)\nprint(f\"The video is predicted to show: {prediction}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-03T14:08:05.882149Z","iopub.execute_input":"2024-10-03T14:08:05.882564Z","iopub.status.idle":"2024-10-03T14:08:07.078979Z","shell.execute_reply.started":"2024-10-03T14:08:05.882528Z","shell.execute_reply":"2024-10-03T14:08:07.078032Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"The video is predicted to show: Shoplifter\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\nimport torch\nimport numpy as np\n\n# Function to evaluate the model on validation/test data and collect predictions and true labels\ndef evaluate_model(dataloader, model, device):\n    model.eval()  # Set the model to evaluation mode\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():  # Disable gradient calculation for evaluation\n        for batch in dataloader:\n            inputs, labels = batch\n\n            # Move inputs and labels to the GPU/CPU\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            # Forward pass to get model predictions\n            outputs = model(pixel_values=inputs)\n            logits = outputs.logits\n\n            # Get predicted class labels\n            preds = torch.argmax(logits, dim=1)\n\n            # Store the predictions and true labels\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    return np.array(all_preds), np.array(all_labels)\n\n# Function to print confusion matrix and classification report\ndef print_evaluation_metrics(dataloader, model, device):\n    # Get predictions and true labels\n    y_pred, y_true = evaluate_model(dataloader, model, device)\n\n    # Print confusion matrix\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_true, y_pred))\n\n    # Print classification report (precision, recall, F1-score)\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_true, y_pred, target_names=['Non-Shoplifter', 'Shoplifter']))\n\n# Example usage\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Assuming you have a DataLoader for your validation or test data\nprint_evaluation_metrics(test_loader, model, device)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T14:08:42.380032Z","iopub.execute_input":"2024-10-03T14:08:42.380401Z","iopub.status.idle":"2024-10-03T14:09:01.714779Z","shell.execute_reply.started":"2024-10-03T14:08:42.380365Z","shell.execute_reply":"2024-10-03T14:09:01.713723Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Confusion Matrix:\n[[91  0]\n [ 0 69]]\n\nClassification Report:\n                precision    recall  f1-score   support\n\nNon-Shoplifter       1.00      1.00      1.00        91\n    Shoplifter       1.00      1.00      1.00        69\n\n      accuracy                           1.00       160\n     macro avg       1.00      1.00      1.00       160\n  weighted avg       1.00      1.00      1.00       160\n\n","output_type":"stream"}]}]}